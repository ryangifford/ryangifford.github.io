[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Gifford",
    "section": "",
    "text": "Welcome to me website…"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sampling_distributions.html",
    "href": "sampling_distributions.html",
    "title": "Sampling Distributions Explained",
    "section": "",
    "text": "This data set includes compliance scores for hospitals throughout the U.S. The goal is to find the mean compliance score for the whole country given a sample.\n\ndat <- read.csv(\"/Users/ryangifford/Desktop/Data Science/Sampling Distributions/compliance_scores.csv\") # read in the data\n\n# define parameters using actual calculations\ntrue_pop_mean <- mean(dat$score)\npop_stdev <- sd(dat$score)\n\n\n\nsampling_distribution <- function(sample_size, trials){\n\ndata <- data.frame(score = integer()) # creates empty data frame\n\nfor(i in 1:trials) {                                   # Head of for-loop\n  new <- mean(dat[sample(nrow(dat), sample_size), 1])      # Create new row (a sample mean)\n  data[nrow(data) + 1, ] <- new                   # Append the new row to existing df\n}\n\n# now we have a df of sample means and can now analyze the distribution of the \n# sample means\n\n\n#############################\n# Confidence Intervals\n############################\n\n# confidence level: 1-(alpha/2). In this case 0.975\n\n\nsample <- data.frame(score = dat[sample(nrow(dat), 30), 1]) # create a sample \nsample_mean <- mean(sample$score) # calculate mean of sample (blue line)\nsample_stdev <- sd(sample$score) # the sd of the sample can be used to estimate sigma\nn <- nrow(sample) # number of observations\nstd_err_mean <- sample_stdev/sqrt(n) # the standard deviation of the distribution of sample means\nt_value <- qt(0.975, 49, lower.tail=TRUE) # find t value. Conf level, degrees of freedom. If this was a normal distribution, the z-score would be 1.96. In other words, 95% of the data would be found within the mean plus/minus 1.96 stdevs.As the degrees of freedom increase, the tails get fatter.\n\nt <- (sample_mean-true_pop_mean)/(std_err_mean) # think of this number, test stat, as the number of standard errors away from the sample mean\n\np <- 2*pt(-abs(t), df=length(sample$score)-1) # p-value\n\nconf_int_plus <-  sample_mean + t_value * (sample_stdev/sqrt(n)) # blue dashed line\n\nconf_int_minus <-  sample_mean - t_value * (sample_stdev/sqrt(n))# cond int. or \n\n# in other words, critical values. The t-value come from the qt() function. \n\n# if the test statistic, t, exceeds the confidence interval, you reject null\n\n\n# the standard error of the mean (x_bar / sqrt(n)) is the 'standard deviation'\n# of the sampling distribution. SEM should approximate sd(data$score) from above\n# function. \n\nggplot(data, aes(x=score)) + \n  geom_histogram(fill = \"grey\") + \n  geom_vline(xintercept = mean(data$score), linetype = \"dashed\") + \n  geom_vline(xintercept = true_pop_mean) +\n  geom_vline(xintercept =  sample_mean, color = \"blue\") +\n  geom_vline(xintercept = conf_int_plus, color = \"steelblue\", linetype = \"dashed\") + \n  geom_vline(xintercept = conf_int_minus, color = \"steelblue\", linetype = \"dashed\") +\n  annotate(\"text\", x = 0, y = 5, label = paste(\"P-value: \", round(p,2), sep = \"\"))\n\n\n\n}\n\n\n\nsampling_distribution(sample_size = 30, trials = 500)\n\n\n\n########################\n#     R T test\n########################\n# ttest <- t.test(sample$score, mu  = true_pop_mean)\n# ttest$statistic\n\nThis plot shows the distribution of sample means.\nEven if the dark blue line is 3 standard errors away from the mean of the sampling distribution of the sample means, it is still within the 95% confidence interval. This is because the confidence interval uses the MSE in the calc and the stdev of the distribution is the MSE. Solid black: truth, dashed black: mean of distribution of sample means, solid blue: sample mean.\nIf we take a sample, the mean of that sample will fall somewhere on the above distribution–lets say the blue line. We know that this distribution will be normal if our sample size is greater than about 30. We also know that the mean of this distribution (dashed black line) will approach the true population mean (solid black line).\nIn a t-test, we estimate what we believe the true population mean is (the black line).This minus the sample mean divided by the mean standard error (the standard deviation of distribution) will give us a t-score (a point on the distribution. The p-value is the probability that we got the sample mean we did given the population mean parameter. If the probability is small (<.05), then the probability is too small for us to get the result we did and we reject the null ( or estimated population mean).\nIn this case, there is a 95% chance the the true pop. mean is contained within the blue lines.\nthe square root in the mse formula is the penalty for using a sample of data not the entire population–thus we use a t-distribution not z."
  },
  {
    "objectID": "index.html#resume",
    "href": "index.html#resume",
    "title": "Ryan Gifford",
    "section": "Resume",
    "text": "Resume\n\n208.313.2514 | gifford.ryan95@gmail.com | linkedin.com/in/giffordryan\n\nEDUCATION\n\nEconomics, B.S. | Brigham Young University - Idaho  Grad"
  },
  {
    "objectID": "index.html#other",
    "href": "index.html#other",
    "title": "Ryan Gifford",
    "section": "Other",
    "text": "Other"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Ryan Gifford",
    "section": "About Me",
    "text": "About Me"
  },
  {
    "objectID": "index.html#css-resume.css",
    "href": "index.html#css-resume.css",
    "title": "Ryan Gifford",
    "section": "css: resume.css",
    "text": "css: resume.css\nWelcome to me website…\n\nSee various reports in the sidebar.\n\n\nAbout Me"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ryan Gifford",
    "section": "Education",
    "text": "Education\n\n\nEconomics, B.S. | Brigham Young University - Idaho\n\n\nGraduated: Apr. 2021\n\n \n\n3.76 GPA, Ezra Taft Benson Excellence in Economics Award\n\n\nMinor in statistics"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Ryan Gifford",
    "section": "Skills",
    "text": "Skills"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Ryan Gifford",
    "section": "Experience",
    "text": "Experience\n\n\nSenior Data Analyst | Healthcare Data Analytics\n\n\nApril 2021 – Current\n\n\n\n\nAssist startup with the implementation of Google’s data platform, Looker, and ensures scaleability\n\nDesign queries and dashboards to show insights from pricing data of over 6,000 US hospitals\n\nUse web development techniques to ensure dashboards are customer friendly, navigable, and visually pleasing\n\nCollaborate with team members and iterate through dev, qa, and prod workflows using Git\n\nAssist Product Manager with up to 5 sales calls/week to provide demonstrations and technical input to hospitals, insurance companies, and healthcare consultants\n\n\nUse Python to automate many BI tasks and create scripts to generate insights from billions of records\n\n\n\n\nData Consultant | Good2Go\n\n\nJan. 2021 – Apr. 2021\n\n\n\n\nWorked closely with executives at Good2Go to explore forecasting techniques and data automation\n\nManipulated two years of transactional data from over 60 convenience stores into datasets for analysis\n\nImplemented time-series forecasting models such as seasonal naïve, ARIMA, and exponential smoothing\n\nDesigned and launched Power BI dashboard that included graphs and tables of sales forecasts, a location map, and model validation/accuracy predictions\n\n\n\n\nProject Manager | Research and Business Development Center\n\n\nApr. 2020 – Apr. 2021\n\n\n\n\nFacilitated research efforts of 5-7 intern teams working on a variety of consulting projects for small startups, governments, schools, and international companies such as Emerson Electric and FXI\n\n\nTrained teams weekly on Agile methodology, competitive analysis, or other essential business skills\n\nResolved team conflicts and roadblocks by maintaining daily contact with team leaders, attending client meetings, providing suggestions and giving correction when necessary\n\nRestructured administrative database to organize documents, spreadsheets, and other training materials\n\n\n\n\nResearch Analyst | Research and Business Development Center\n\n\nSep. 2019 – Dec. 2019\n\n\n\n\nUtilized Excel to analyze large amounts of data and used findings to propose high school curriculum suggestions to city officials, a school board, and state legislatures\n\n\nCreated an Excel tool to automate and simplify annexation processes for the City of Idaho Falls\n\nCollaborated well with various teams, clients, and supervisors and in turn was offered a paid position"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Ryan Gifford",
    "section": "Interests",
    "text": "Interests\n\nBiking\n\n\n\n\n\nMusic"
  }
]